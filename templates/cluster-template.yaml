---
apiVersion: cluster.x-k8s.io/v1beta2
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
spec:
  clusterNetwork:
    services:
      cidrBlocks: ${SERVICE_CIDR:=["10.128.0.0/12"]}
    pods:
      cidrBlocks: ${POD_CIDR:=["192.168.0.0/16"]}
    serviceDomain: ${SERVICE_DOMAIN:="cluster.local"}
  controlPlaneEndpoint:
    host: ${LIBVIRT_CONTROL_PLANE_ENDPOINT_HOST}
    port: ${LIBVIRT_CONTROL_PLANE_ENDPOINT_PORT:=6443}
  controlPlaneRef:
    apiGroup: controlplane.cluster.x-k8s.io
    kind: KubeadmControlPlane
    name: ${CLUSTER_NAME}
  infrastructureRef:
    apiGroup: infrastructure.cluster.x-k8s.io
    kind: LibvirtCluster
    name: ${CLUSTER_NAME}

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: LibvirtCluster
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: LibvirtMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-control-plane
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      network: "${LIBVIRT_MACHINE_NETWORK}"
      storagePool: "${LIBVIRT_MACHINE_STORAGE_POOL}"
      cpu: ${LIBVIRT_CONTROL_PLANE_CPU:=2}
      memory: ${LIBVIRT_CONTROL_PLANE_MEMORY:=3072}
      diskSize: ${LIBVIRT_CONTROL_PLANE_DISK_SIZE:=20}
      backingImagePath: ${LIBVIRT_MACHINE_BACKING_IMAGE}

---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: LibvirtMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-worker
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      network: "${LIBVIRT_MACHINE_NETWORK}"
      storagePool: "${LIBVIRT_MACHINE_STORAGE_POOL}"
      cpu: ${LIBVIRT_WORKER_CPU:=1}
      memory: ${LIBVIRT_WORKER_MEMORY:=1536}
      diskSize: ${LIBVIRT_WORKER_DISK_SIZE:=20}
      backingImagePath: ${LIBVIRT_MACHINE_BACKING_IMAGE}

---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: KubeadmControlPlane
metadata:
  name: ${CLUSTER_NAME}
  namespace: ${NAMESPACE}
spec:
  machineTemplate:
    spec:
      infrastructureRef:
        apiGroup: infrastructure.cluster.x-k8s.io
        kind: LibvirtMachineTemplate
        name: ${CLUSTER_NAME}-control-plane
  replicas: ${CONTROL_PLANE_MACHINE_COUNT:=1}
  version: "${KUBERNETES_VERSION}"
  kubeadmConfigSpec:
    format: cloud-config
    users:
    - name: "${LIBVIRT_SSH_USER:=clusteradmin}"
      sshAuthorizedKeys:
      - '${LIBVIRT_SSH_PUBLIC_KEY}'
      sudo: ALL=(ALL) NOPASSWD:ALL
      shell: /bin/bash
    clusterConfiguration:
      apiServer:
        certSANs:
        - "127.0.0.1"
        - "localhost"
    initConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
        - name: provider-id
          value: libvirt:///{{ local_hostname }}
        name: '{{ local_hostname }}'
    joinConfiguration:
      nodeRegistration:
        kubeletExtraArgs:
        - name: provider-id
          value: libvirt:///{{ local_hostname }}
        name: '{{ local_hostname }}'
    files:
    - content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-vip
          namespace: kube-system
        spec:
          containers:
          - args:
            - manager
            env:
            - name: address
              value: ${LIBVIRT_CONTROL_PLANE_ENDPOINT_HOST}
            - name: vip_arp
              value: "true"
            - name: port
              value: "${LIBVIRT_CONTROL_PLANE_ENDPOINT_PORT:=6443}"
            # - name: vip_interface
            #   value: ens192 # allow kube-vip to auto-detect the interface
            - name: vip_subnet
              value: "32"
            - name: cp_enable
              value: "true"
            - name: cp_namespace
              value: kube-system
            - name: vip_ddns
              value: "false"
            - name: svc_enable
              value: "true"
            - name: vip_leaderelection
              value: "true"
            - name: vip_leaseduration
              value: "5"
            - name: vip_renewdeadline
              value: "3"
            - name: vip_retryperiod
              value: "1"
            image: ghcr.io/kube-vip/kube-vip:v1.0.3
            imagePullPolicy: IfNotPresent
            name: kube-vip
            resources: {}
            securityContext:
              capabilities:
                add:
                - NET_ADMIN
                - NET_RAW
                - SYS_TIME
            volumeMounts:
            - mountPath: /etc/kubernetes/admin.conf
              name: kubeconfig
              readOnly: true
            - mountPath: /etc/kubernetes/pki
              name: pki
              readOnly: true
          hostAliases:
          - hostnames:
            - kubernetes
            ip: 127.0.0.1
          hostNetwork: true
          volumes:
          - hostPath:
              path: /etc/kubernetes/kube-vip.conf
              type: File
            name: kubeconfig
          - hostPath:
              path: /etc/kubernetes/pki
              type: Directory
            name: pki
          tolerations:
          - effect: NoExecute
            operator: Exists
          - effect: NoSchedule
            operator: Exists
      owner: root:root
      path: /etc/kubernetes/manifests/kube-vip.yaml
      permissions: "0644"

    - content: 127.0.0.1 localhost kubernetes
      owner: root:root
      path: /etc/kube-vip.hosts
      permissions: "0644"

    # Create a temporary kubeconfig for kube-vip - it will be updated during kubeadm init
    - content: |
        apiVersion: v1
        kind: Config
        clusters:
        - name: local
          cluster:
            server: https://127.0.0.1:6443
            # insecure-skip-tls-verify: true
        users:
        - name: placeholder
          user: {}
        contexts:
        - name: default
          context:
            cluster: local
            user: placeholder
        current-context: default
      owner: root:root
      path: /etc/kubernetes/kube-vip.conf
      permissions: "0644"

    preKubeadmCommands:
    - echo "::1         ipv6-localhost ipv6-loopback localhost6 localhost6.localdomain6" >>/etc/hosts
    - echo "127.0.0.1   {{ local_hostname }} localhost localhost.localdomain localhost4 localhost4.localdomain4" >>/etc/hosts
    # Start a background process to update kube-vip.conf once admin.conf exists
    # This allows kube-vip to get proper credentials DURING kubeadm init
    - |
      (
        while [ ! -f /etc/kubernetes/admin.conf ]; do
          sleep 2
        done
        KUBECONFIG_ADMIN=/etc/kubernetes/admin.conf
        # If super-admin.conf exists, use that instead
        if [ -f /etc/kubernetes/super-admin.conf ]; then
          KUBECONFIG_ADMIN=/etc/kubernetes/super-admin.conf
        fi
        sed 's|server:.*|server: https://127.0.0.1:6443|' $KUBECONFIG_ADMIN > /etc/kubernetes/kube-vip.conf
        chmod 644 /etc/kubernetes/kube-vip.conf
      ) &

    postKubeadmCommands:
    # Install CNI
    - KUBECONFIG=/etc/kubernetes/super-admin.conf kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

---
apiVersion: cluster.x-k8s.io/v1beta2
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-worker
  namespace: ${NAMESPACE}
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT:=1}
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  template:
    metadata:
      labels:
        cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
    spec:
      version: "${KUBERNETES_VERSION}"
      clusterName: ${CLUSTER_NAME}
      bootstrap:
        configRef:
          apiGroup: bootstrap.cluster.x-k8s.io
          kind: KubeadmConfigTemplate
          name: ${CLUSTER_NAME}-worker
      infrastructureRef:
        apiGroup: infrastructure.cluster.x-k8s.io
        kind: LibvirtMachineTemplate
        name: ${CLUSTER_NAME}-worker

---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
kind: KubeadmConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-worker
  namespace: ${NAMESPACE}
spec:
  template:
    spec:
      format: cloud-config
      users:
      - name: "${LIBVIRT_SSH_USER:=clusteradmin}"
        sshAuthorizedKeys:
        - '${LIBVIRT_SSH_PUBLIC_KEY}'
        sudo: ALL=(ALL) NOPASSWD:ALL
        shell: /bin/bash
      joinConfiguration:
        nodeRegistration:
          name: '{{ local_hostname }}'
          kubeletExtraArgs:
          - name: provider-id
            value: libvirt:///{{ local_hostname }}
          - name: "eviction-hard"
            value: "nodefs.available<0%,nodefs.inodesFree<0%,imagefs.available<0%"
